# Capstone Portfolio

This portfolio highlights the Data Science Capstone project, focusing on leveraging advanced data analysis and machine learning techniques to solve a specific challenge. Click the link below to view the code on my personal GitHub page.

## [Group6Capstone: Project Overview](https://github.com/SamRobinson123/Group6Capstone/blob/main/Group6Capstone.ipynb)
![Project Status](https://img.shields.io/badge/status-complete-green.svg)

### Business Problem
The **Group6Capstone** project aims to analyze data and build predictive models to address challenges in [insert domain or sector, e.g., financial forecasting, healthcare analytics, etc.]. The projectâ€™s main focus is to ensure that data-driven insights can be effectively leveraged to [specific goal, e.g., improve financial performance, enhance decision-making, or optimize operations]. The project applies modern machine learning techniques to uncover trends, make accurate predictions, and drive actionable recommendations.

### ðŸŽ¯ Objective
The primary goal of this project was to:
- [Insert objective, e.g., "Develop a predictive model for customer segmentation," or "Build a time series model for financial forecasting."]
- Leverage exploratory data analysis (EDA) to uncover actionable insights.
- Implement machine learning models to solve [specific problem].

### ðŸ“Š Project Details
- **Dataset**: The project utilized a comprehensive dataset containing [insert data points/records], capturing information on [brief description of the data, e.g., transactions, customer behavior, etc.].
- **Data Cleaning**: Applied robust preprocessing techniques, including:
  - Handling missing values and duplicates.
  - Addressing outliers using statistical methods.
  - Transforming variables for better modeling performance.
- **Modeling**:
  - Developed a [insert model name, e.g., Random Forest, XGBoost, etc.] model to tackle the problem.
  - Achieved the following metrics:
    - **Accuracy**: [Insert value, e.g., 92%].
    - **Other Metric (e.g., AUC)**: [Insert value, e.g., 0.85].
  - Models were evaluated using cross-validation to ensure generalizability.
 
### ðŸš€ Personal Contribution
As a member of the **Group6Capstone** team, I contributed significantly to the project through the following efforts:

1. **Building Imputation Models**:
   - Developed **CatBoost** and **Random Forest Regressor** models to impute missing values in key dataset features. These models were carefully tuned to ensure accuracy and robustness in predicting missing data points.

2. **Critical Decision-Making**:
   - After evaluating the models' performance and data integrity, I led the decision not to use these imputation methods. This decision was based on the observation that only 20% of the dataset contained viable data, making imputation potentially unreliable. This decision ensured the overall integrity and reliability of the projectâ€™s final analysis.

3. **Collaborative EDA and Modeling**:
   - Assisted in uncovering insights during EDA and contributed to feature engineering for downstream modeling efforts.

### ðŸš€ Value of the Solution
This project delivered valuable insights and predictions, leading to:
- Improved accuracy in [specific area, e.g., financial forecasting or customer segmentation].
- Enhanced decision-making capabilities by leveraging machine learning techniques.
- A foundation for scaling predictive models to solve similar challenges in related domains.

### Key Metrics
- **Performance**: Achieved an average AUC score of [insert AUC score, e.g., 0.77].
- **Accuracy**: Achieved a prediction accuracy of [insert accuracy, e.g., 91%].

These metrics highlight the effectiveness of the approach and its applicability to real-world challenges.

### Challenges

The project faced several significant challenges, including:

1. **High Proportion of Missing Data**:
   - Over 80% of the dataset contained null values, making it difficult to perform robust analyses and train reliable models without introducing potential biases.

2. **Unclear Target Variable**:
   - The definition and consistency of the target variable were ambiguous, requiring additional preprocessing and domain-specific assumptions to ensure accurate and meaningful predictions.

These challenges underscored the importance of meticulous data preprocessing, critical decision-making, and domain expertise in overcoming obstacles to achieve project success.

### Takeaways
Key learnings from this project include:
- The critical importance of data preprocessing in machine learning workflows.
- How using modularized functions and classes improves code maintainability and reduces errors.
- The significant value of hyperparameter tuning to achieve optimal model performance.
